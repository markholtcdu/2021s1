<!DOCTYPE html>
<html>
<head>
  <link rel ="stylesheet" href="prabeshstyle.css">
  <meta charset ="utf-8">
  <title> Webpage of Prabesh </title>
</head>
<body>
   <li ><a href="Prabesh.html"> Notes Page</a></li>
  <li ><a href="Introduction.html"> Introduction</a></li>
  <li ><a href="Dead.html"> Dead</a></li>
  <li ><a href="Phrase.html">Phrase</a></li>
  <li><a href="Parse.html">Parse </a></li>
    
    <h1>  Natural Language Processing </h1>
     </header>
      <article id="1">
        <h2>Phrase Labelling task</h2>
     
 <p class = "box"> to sum up the grammar of a language </p>
        <p class = "box">it makes set of rules for grammar </p>
        <p class = "box"> PARSER helps the student to learn English structure </p>
  <p class = "box">John says phraseFlow is how to further enlarge the input chunk
to phrase level, as multiple words are corrected through one operation. Studies in human factors and psychology tended to fnd word
as the basic chunk of typing. </p> 
  </article>
    <article id ="2">
      <h2>Parse Tree </h2>
       <p class = "box">likely to attach every word and reveals how sentence constructed   </p>
         <p class = "box">to help to allow computers to access, process and respond to information </p>
     <p class = "box"> Ford introduced Parsing Expression Grammars (PEGs) as a formalism for defining unambiguous grammars that can be parsed efficiently using a recursive descent
scheme with parsing combinators for the basic grammar
operators.  </p>
      <p class ="box">Bratus says for critical applications, parsing errors and lack of
proper input validation can be a common source of vulnerability and inconsistency leading to numerous errors and
attacks. </p>
    </article>
    <article id="3">
      <h2> Dead languages come to life </h2>
       <p class = "box">Luo says commercial systems like
Google Translate work at the semantic
level, converting entire sentences from
one language to another in a way that
tries to preserve their meaning </p>
      <p class = "box"> Berg-Kirkpatrick says explicitly pose the problem of cognate matching as a combinatorial optimization
problem, and conceptually divide the
problem into alphabet- and cognatematching. What their paper does that’s
quite new is make use of high-capacity
neural nets within this framework. It
works quite wel</p>
  </article>
 
      <footer> 
        <h2> References </h2>
     
          <li>"Bailin, Alan, and Philip Thomson. "The Use of Natural Language Processing in Computer-Assisted Language Instruction." Computers and the Humanities 22, no. 2 (1988): 99-110."  </li>
        <li> "B. E. John and A. Newell. 1989. Cumulating the Science of HCI: From s-R
    Compatibility to Transcription Typing. SIGCHI Bull. 20, SI (March 1989), 109–114"  </li>
          <li>"Bryan Ford. 2004. Parsing expression grammars: A recognition-based
syntactic foundation. In Proceedings of the 31st ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, POPL 2004, Venice,
Italy, January 14-16, 2004, Neil D. Jones and Xavier Leroy (Eds.). ACM,
111–122" </li>
          <li>  "Sergey Bratus, Lars Hermerschmidt, Sven M. Hallberg, Michael E.
Locasto, Falcon Momot, Meredith L. Patterson, and Anna Shubina.
2017. Curing the vulnerable parser: Design patterns for secure input
            handling. ;login: 42, 1 (2017), 33–39" </li>
          <li> "Luo, J. Cao, Y. and Barzilay, R.
Neural Decipherment via Minimum-Cost
Flow: from Ungaritic to Linear B, eprint
            arXiv:1906.06718, June 2019"  </li>
          <li>  "Berg-Kirkpatrick, T. and Klein, D.
Simple Effective Decipherment via
combinatorial Optimization, Proceedings of
the 2011 Conference on Empirical Methods
            in Natural Language Processing, July 2011" </li>
   
          </footer>
    </body>
  </html>
